{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Boosting Techniques Assignment"
      ],
      "metadata": {
        "id": "urts2IP6SAJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Assignment Questions\n",
        "\n",
        "\n",
        "\n",
        "-- Assignment Code: DA-AG-015"
      ],
      "metadata": {
        "id": "hm3dLKpUm-sO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q1) What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
        "\n",
        "Boosting is an ensemble technique that builds a strong predictive model by sequentially training many weak learners (often shallow decision trees) where each new learner focuses on the mistakes of the previous ones. Instead of training models independently, boosting assigns higher weight to misclassified examples (or fits the negative gradients of a chosen loss), forcing subsequent learners to concentrate on hard cases. The final prediction is a weighted sum (or vote) of the weak learners’ outputs. Boosting reduces bias (and often variance), can learn complex patterns, and typically yields high accuracy when properly regularized. However, it is more sensitive to noisy labels and outliers and can overfit without regularization/early stopping."
      ],
      "metadata": {
        "id": "_Paa8myZvgHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "22jmk71F-lNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q2) : What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "\n",
        "AdaBoost trains weak learners sequentially by reweighting training examples: after each learner, it increases weights of misclassified samples so the next learner focuses on them; final predictions are a weighted vote of learners. AdaBoost effectively minimizes an exponential loss. Gradient Boosting, by contrast, trains learners to fit the negative gradient (pseudo-residuals) of a differentiable loss function — each new learner models the residual errors of the ensemble so far; predictions update by adding the learner scaled by a learning rate. Gradient Boosting is more flexible (any differentiable loss), supports shrinkage (learning rate), and is the basis for modern boosted-tree libraries (XGBoost, LightGBM, CatBoost).\n"
      ],
      "metadata": {
        "id": "NQ4qB9bSvp-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "azZUk2uQ-t1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q3) How does regularization help in XGBoost?\n",
        "\n",
        "XGBoost incorporates multiple regularization mechanisms to prevent overfitting and improve generalization: L1 (Lasso) and L2 (Ridge) penalties on leaf weights shrink large weights; gamma (minimum loss reduction) requires a minimum improvement for a split; max_depth, min_child_weight, subsample, colsample_bytree/colsample_bylevel limit tree complexity and sampling; learning_rate (shrinkage) scales new tree contributions; and lambda/alpha are the L2/L1 regularization terms. Together these penalties control model complexity, reduce variance, stabilize learning over noisy data, and improve robustness. XGBoost also supports early stopping on a validation set to prevent overfitting."
      ],
      "metadata": {
        "id": "NviEDIScv-Xr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Vojnm6kT-vL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q4) Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "CatBoost is designed specifically to handle categorical features natively — you pass categorical column indices and CatBoost applies robust encodings (ordered target statistics, permutations) that avoid target leakage. It uses ordered boosting to reduce prediction shift and bias, which improves generalization on categorical-rich tabular data. CatBoost also implements efficient handling for high-cardinality features, supports symmetric trees for faster inference, and provides GPU acceleration. Because it removes the need for manual target encoding/one-hot explosion and reduces leakage risk, CatBoost often outperforms other models on datasets with many categorical variables.\n"
      ],
      "metadata": {
        "id": "Q9yqVwnawHIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "spxMThJP-wP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q5 - What are some real-world applications where boosting techniques are preferred over bagging methods?\n",
        "\n",
        "Boosting is preferred when maximum predictive accuracy is needed and when patterns are subtle or biased, e.g.: credit scoring and fraud detection, customer churn prediction, targeted marketing (response modeling), demand forecasting, risk modeling in finance/insurance, and many Kaggle/tabular competition problems. Boosting often outperforms bagging on tabular data because it reduces bias and can learn complex interactions. It’s especially useful for imbalanced problems when tuned properly (with appropriate evaluation metrics and class-weighting). The tradeoff: boosting is slower to train, more sensitive to noisy labels, and requires careful regularization."
      ],
      "metadata": {
        "id": "nZ5dL8h1wLA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "sOwacEDX-xku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q6) : Write a Python program to:\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "● Print the model accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "Hjv6cjB6xCkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# AdaBoost uses Decision Stumps by default (good baseline)\n",
        "adb = AdaBoostClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "adb.fit(X_train, y_train)\n",
        "y_pred = adb.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, digits=4))"
      ],
      "metadata": {
        "id": "beLvsC79iode"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Y3ERHrKi-zOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q7) Write a Python program to:\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "\n",
        "● Evaluate performance using R-squared score"
      ],
      "metadata": {
        "id": "sdv5g2iJxG3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Try to fetch California Housing; fallback to diabetes if unavailable\n",
        "try:\n",
        "    data = fetch_california_housing()\n",
        "    X, y = data.data, data.target\n",
        "except Exception as e:\n",
        "    print(\"fetch_california_housing failed, falling back to diabetes dataset. Error:\", e)\n",
        "    from sklearn.datasets import load_diabetes\n",
        "    data = load_diabetes()\n",
        "    X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "gbr = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gbr.fit(X_train, y_train)\n",
        "y_pred = gbr.predict(X_test)\n",
        "print(\"R^2 score:\", r2_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "85325a9Uiw92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SiOG6P6G-5xt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q8) Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "● Tune the learning rate using GridSearchCV\n",
        "\n",
        "● Print the best parameters and accuracy\n"
      ],
      "metadata": {
        "id": "Cu-SG0ljxKga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install xgboost\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "except ImportError:\n",
        "    raise ImportError(\"Please install xgboost: pip install xgboost\")\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1)\n",
        "\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 6],\n",
        "    'n_estimators': [50, 100, 200]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(xgb, param_grid, cv=4, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "best = grid.best_estimator_\n",
        "print(\"Test accuracy:\", accuracy_score(y_test, best.predict(X_test)))"
      ],
      "metadata": {
        "id": "JebRDrcZjBOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "L6sNglT2-67-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q9 - Write a Python program to:\n",
        "● Train a CatBoost Classifier\n",
        "\n",
        "● Plot the confusion matrix using seaborn"
      ],
      "metadata": {
        "id": "BJRWsR6pxW92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install catboost seaborn\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostClassifier\n",
        "except ImportError:\n",
        "    raise ImportError(\"Please install catboost: pip install catboost\")\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "cat = CatBoostClassifier(iterations=200, learning_rate=0.05, depth=6, verbose=0, random_state=42)\n",
        "cat.fit(X_train, y_train)\n",
        "\n",
        "y_pred = cat.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# Confusion matrix plot\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['neg','pos'], yticklabels=['neg','pos'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('CatBoost Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HfMS8fr1jGPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1eLXPx4l-74Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q10 - You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior.The dataset is imbalanced, contains missing values, and has both numeric and categorical features.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "\n",
        "\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "● Hyperparameter tuning strategy\n",
        "\n",
        "● Evaluation metrics you'd choose and why\n",
        "\n",
        "● How the business would benefit from your model"
      ],
      "metadata": {
        "id": "HNRp-mw2xmMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Answer: Step-by-step pipeline summary\n",
        "\n",
        "EDA & target definition: examine class imbalance, missingness patterns, feature distributions, correlations, and business costs (FN vs FP).\n",
        "\n",
        "Missing values: for numeric features use SimpleImputer (median or model-based imputation); for categorical use SimpleImputer(strategy='constant', fill_value='missing') or CatBoost’s native support. Create missing-indicator features to capture informative missingness.\n",
        "\n",
        "Categorical features: Prefer CatBoost (native handling) OR for XGBoost/LightGBM use one-hot for low-cardinality and target/mean encoding (with regularization and CV) or frequency encoding for high-cardinality features. Beware target leakage.\n",
        "\n",
        "Feature engineering: add aggregates from transactions (avg transaction, max, recency), behavioral flags, transaction counts, ratios, and time-based features. Normalize only if using non-tree models.\n",
        "\n",
        "Class imbalance: compute class weights or set scale_pos_weight in XGBoost; threshold tuning; prefer cost-sensitive learning over blind resampling for transactional data. If using resampling, use SMOTE/ADASYN carefully (synthetic features for numeric-only parts), or stratified oversampling.\n",
        "\n",
        "Model choice: start with CatBoost (native categorical support + ordered boosting) or XGBoost/LightGBM (fast, powerful). AdaBoost is rarely best for heavily imbalanced/tabular complex datasets.\n",
        "\n",
        "Hyperparameter tuning: use RandomizedSearchCV or Bayesian (Optuna) on parameters like learning_rate, max_depth, n_estimators, subsample, colsample_bytree, min_child_weight, and regularization lambda/alpha; use early stopping on validation set. Use stratified CV and scoring aligned to business metric (e.g., recall@precision threshold).\n",
        "\n",
        "Evaluation metrics: prefer Precision, Recall, F1, PR-AUC (average precision), and ROC-AUC; evaluate calibration (Brier score) if probabilities are acted on. Use confusion-matrix and cost-sensitive metrics (expected monetary loss). Monitor fairness and feature leakage.\n",
        "\n",
        "Model explainability & deployment: compute SHAP values to explain predictions for adjudication and compliance. Establish monitoring for drift and periodic retraining. Use conservative thresholds and human review for borderline/high-impact decisions.\n",
        "\n",
        "Business impact: reduce default losses by early flagging of high-risk applicants, improving decision automation and risk-based pricing, and enabling targeted interventions (e.g., reminders, different terms) — while maintaining low false-positive rates to avoid rejecting good customers."
      ],
      "metadata": {
        "id": "YoHTkxtgmUkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This uses sklearn's make_classification to simulate data, introduces missing values,\n",
        "# demonstrates imputation, computes class_weight/scale_pos_weight, and trains CatBoost/XGBoost.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, average_precision_score\n",
        "\n",
        "# Generate synthetic dataset (numeric + categorical)\n",
        "X_num, y = make_classification(n_samples=5000, n_features=10, n_informative=5,\n",
        "                               weights=[0.95, 0.05], flip_y=0.01, random_state=42)\n",
        "\n",
        "df = pd.DataFrame(X_num, columns=[f\"num_{i}\" for i in range(X_num.shape[1])])\n",
        "# Add synthetic categorical features\n",
        "np.random.seed(42)\n",
        "df['cat_1'] = np.random.choice(['A','B','C'], size=len(df), p=[0.7,0.2,0.1])\n",
        "df['cat_2'] = np.random.choice(['X','Y'], size=len(df), p=[0.85,0.15])\n",
        "\n",
        "# Introduce missing values randomly\n",
        "for col in ['num_1', 'num_3', 'cat_1']:\n",
        "    df.loc[df.sample(frac=0.05, random_state=42).index, col] = np.nan\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2,\n",
        "                                                    stratify=y, random_state=42)\n",
        "\n",
        "# Preprocessing for XGBoost approach (numerics + one-hot for small-cardinality cats)\n",
        "numeric_features = [c for c in df.columns if c.startswith('num_')]\n",
        "categorical_features = ['cat_1', 'cat_2']\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median'))\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('cat', categorical_transformer, categorical_features)\n",
        "], remainder='drop')\n",
        "\n",
        "# XGBoost pipeline (if xgboost installed)\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    clf_xgb = Pipeline(steps=[\n",
        "        ('preproc', preprocessor),\n",
        "        ('clf', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1))\n",
        "    ])\n",
        "    # compute scale_pos_weight = n_neg / n_pos\n",
        "    n_pos = sum(y_train==1)\n",
        "    n_neg = sum(y_train==0)\n",
        "    scale_pos_weight = n_neg / max(1, n_pos)\n",
        "    clf_xgb.set_params(clf__scale_pos_weight=scale_pos_weight, clf__n_estimators=200, clf__learning_rate=0.05)\n",
        "    clf_xgb.fit(X_train, y_train)\n",
        "    y_prob = clf_xgb.predict_proba(X_test)[:,1]\n",
        "    print(\"XGBoost ROC-AUC:\", roc_auc_score(y_test, y_prob))\n",
        "    print(\"XGBoost Average Precision (PR-AUC):\", average_precision_score(y_test, y_prob))\n",
        "    print(classification_report(y_test, clf_xgb.predict(X_test), digits=4))\n",
        "except Exception as e:\n",
        "    print(\"XGBoost not available or failed:\", e)\n",
        "\n",
        "# CatBoost approach (native categorical handling)\n",
        "try:\n",
        "    from catboost import CatBoostClassifier, Pool\n",
        "    cat_features_idx = [X_train.columns.get_loc(c) for c in categorical_features]\n",
        "    # CatBoost accepts DataFrame directly\n",
        "    model_cat = CatBoostClassifier(iterations=500, learning_rate=0.05, depth=6, random_state=42,\n",
        "                                   class_weights=[1, n_neg/n_pos], verbose=0)\n",
        "    model_cat.fit(X_train, y_train, cat_features=categorical_features, eval_set=(X_test, y_test), use_best_model=True)\n",
        "    y_prob_cat = model_cat.predict_proba(X_test)[:,1]\n",
        "    print(\"CatBoost ROC-AUC:\", roc_auc_score(y_test, y_prob_cat))\n",
        "    print(\"CatBoost Average Precision (PR-AUC):\", average_precision_score(y_test, y_prob_cat))\n",
        "    print(classification_report(y_test, model_cat.predict(X_test), digits=4))\n",
        "except Exception as e:\n",
        "    print(\"CatBoost not available or failed:\", e)"
      ],
      "metadata": {
        "id": "owkIJ03VjeHs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}